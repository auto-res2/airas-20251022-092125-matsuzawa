
LLM Name: o3-2025-04-16
Input:
You are an AI research assistant tasked with generating Hydra configuration files for experiment runs.

# Task
Generate individual YAML configuration files for each experiment run. These configs will be used by Hydra to configure specific experimental variations.

# Input Information

## Research Method
{
    "Open Problems": "In BOIL the trade-off between expected utility (EI) and training cost uses a plain LinearRegression model that outputs only the cost mean.  • Learning curves in practice exhibit highly non-linear, heteroscedastic cost patterns (e.g. GPU time depends on batch-size, network width, data-set size, etc.).  • Ignoring cost uncertainty can mis-rank candidate points and waste optimisation budget.  A minimal change that adds uncertainty estimates to the cost model can address this limitation without touching BOIL’s learning-curve GP or augmentation machinery.",
    "Methods": "We keep BOIL intact but replace its cost proxy with a BayesianRidge regressor and modify the acquisition so that the denominator is the optimistic cost mean augmented by a multiple of its predictive standard deviation.  Concretely:  1. Fit sklearn.linear_model.BayesianRidge on concatenated (x,t) → observed_cost.  2. At a candidate z=[x,t] obtain posterior mean μ_c and std σ_c.  3. New cost-aware acquisition:  A(z)=log(EI(z))−log( μ_c + β·σ_c + ε )  (ε=1e−6).  Setting β∈[0,2] interpolates between risk-neutral (β=0, original BOIL) and risk-averse (β>0).  Motivation: penalise points whose cost is both high and uncertain, leading to more reliable wall-clock savings while changing only 5 lines of code.",
    "Experimental Setup": "Data sets: CIFAR-10 CNN (same as BOIL repo) and CartPole-v0 DQN.  Budget: 50 BO iterations, 5 random seeds.  Methods compared: (i) Original BOIL, (ii) BOIL-UC (β=1).  Metrics:  • Wall-clock minutes until the incumbent reaches 90% of best-known test accuracy / return.  • Area-under-curve of best-so-far metric vs time.  Hardware and other hyper-parameters identical to BOIL paper to ensure fair comparison.",
    "Experimental Code": "# --- only the changed parts ---\nfrom sklearn.linear_model import BayesianRidge\n\nclass BOIL_UC(BOIL):\n    def __init__(self,*args, beta_uncert=1.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.beta_uncert = beta_uncert\n        # replace linear cost model\n        self.cost_model = BayesianRidge()\n\n    # override utility-cost computation\n    def utility_cost_evaluation(self, z, acq_func, isDebug=False):\n        def _single(z):\n            util = acq_func.acq_kind(z, gp=self.gp)               # EI part (unchanged)\n            mu_c, std_c = self.cost_model.predict(z.reshape(1,-1), return_std=True)\n            adj_cost = max(0.0, mu_c) + self.beta_uncert*std_c + 1e-6\n            return np.log(util) - np.log(adj_cost)\n        if z.ndim==1:\n            return -_single(z)\n        return -np.asarray([_single(p) for p in z])\n\n    # call after every true evaluation\n    def update_cost_model(self):\n        Z = np.hstack((self.X, self.T))\n        self.cost_model.fit(Z, self.Y_cost)\n\n    # ensure model update before acquisition optimisation\n    def suggest_nextpoint(self):\n        self.update_cost_model()\n        return super().suggest_nextpoint()\n",
    "Expected Result": "Across both benchmarks BOIL-UC should reach the target performance about 10-20 % faster in wall-clock time.  The AUC metric is expected to be higher (lower is better) for BOIL-UC in ≥4 of 5 seeds, indicating more consistent cost-aware choices.  When β is set to 0 we exactly reproduce original BOIL numbers, confirming minimal overhead.",
    "Expected Conclusion": "Adding a Bayesian cost model with uncertainty weighting is a tiny change (replace LinearRegression, adjust 5 code lines) yet yields tangible speed-ups and stabler optimisation.  The improvement arises because the acquisition now avoids hyper-parameters whose training cost is both high and uncertain, focusing exploration on cheaper, safer regions.  This demonstrates that even in sophisticated BO frameworks, incorporating predictive uncertainty on auxiliary objectives can provide outsized practical gains with negligible implementation effort."
}

## Experimental Design
{'experiment_summary': 'We compare the original BOIL (Bayesian Optimisation for Learning-curves) against BOIL-UC, our drop-in replacement that equips BOIL with a BayesianRidge cost model and an uncertainty-aware acquisition denominator.  Both algorithms optimise hyper-parameters for training a fixed ResNet-18 on CIFAR-10.  In each BO iteration the candidate (network width, batch-size, learning-rate, etc.) is selected, its single training run is launched, resulting learning-curve observations update BOIL’s GP (utility) and the BayesianRidge (cost).  The acquisition used by BOIL-UC is A(z)=log(EI(z))−log(μ_c+β·σ_c+1e-6).  Experiments run for 50 BO iterations under five random seeds on one A100 GPU.  Wall-clock time until the best validation accuracy reaches 90 % of the global optimum and the time-AUC of best-so-far accuracy are recorded.  The whole workflow is automated via the existing BOIL code-base; only ~15 lines are added to swap the cost model, compute μ_c,σ_c and update β.  Hyper-parameter β is tuned by grid search and the best value is used for the final comparison.', 'evaluation_metrics': ['Time-to-Target (minutes)', 'AUC_BestSoFar'], 'proposed_method': 'BOIL-UC (Bayesian Optimistic Cost).  Objective: accelerate wall-clock convergence of Bayesian hyper-parameter optimisation by accounting for both mean and uncertainty of training cost.\nComponents:\n1. Utility model (unchanged): Gaussian Process on learning-curve features to yield Expected Improvement EI(z).\n2. Cost model (new): sklearn.linear_model.BayesianRidge trained on concatenated design-time features (x,t) → observed GPU minutes.  Gives posterior mean μ_c and std σ_c.\n3. Acquisition: A(z)=log(EI(z))−log(μ_c+βσ_c+ε) where ε=1e-6.  β∈[0,2] trades risk-neutral for risk-averse behaviour.\n4. Algorithmic steps per BO iteration:\n   a. Fit/update GP and BayesianRidge with all past data.\n   b. Optimise A(z) with multistart L-BFGS to choose next configuration.\n   c. Train the candidate model until time t, record validation accuracy, training cost.\n   d. Append observations and repeat.\nTheoretically, incorporating σ_c makes A(z) proportional to an upper-confidence bound on 1/cost, reducing probability of selecting expensive or uncertain runs.  Implementation requires replacing LinearRegression with BayesianRidge, calling predict(...,return_std=True) and adding βσ_c in the denominator.', 'comparative_methods': ['Original BOIL'], 'models_to_use': ['ResNet-18 (11.7 M)'], 'datasets_to_use': ['CIFAR-10'], 'hyperparameters_to_search': {'beta_uncert': '0,0.5,1,2', 'bayesianridge_alpha_1': '1e-7-1e-5', 'bayesianridge_lambda_1': '1e-7-1e-5'}, 'external_resources': {'hugging_face': {'models': [{'id': 'microsoft/resnet-18', 'author': 'microsoft', 'sha': '65a5785d9156231087c481e0c7dd33a5ff6f7e3e', 'created_at': datetime.datetime(2022, 3, 16, 15, 40, 26, tzinfo=TzInfo(UTC)), 'last_modified': datetime.datetime(2024, 4, 8, 11, 6, 50, tzinfo=TzInfo(UTC)), 'private': False, 'gated': False, 'disabled': False, 'downloads': 127325, 'likes': 59, 'siblings': [{'rfilename': '.gitattributes', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'README.md', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'config.json', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'model.safetensors', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'preprocessor_config.json', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'pytorch_model.bin', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'tf_model.h5', 'size': None, 'blob_id': None, 'lfs': None}], 'card_data': {'license': 'apache-2.0', 'language': [], 'library_name': None, 'pipeline_tag': None, 'tags': ['vision', 'image-classification'], 'datasets': ['imagenet-1k'], 'model_type': None, 'base_model': None, 'task_categories': [], 'size_categories': [], 'metrics': [], 'widget': [{'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg', 'example_title': 'Tiger'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg', 'example_title': 'Teapot'}, {'src': 'https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg', 'example_title': 'Palace'}]}, 'tags': ['transformers', 'pytorch', 'tf', 'safetensors', 'resnet', 'image-classification', 'vision', 'dataset:imagenet-1k', 'arxiv:1512.03385', 'license:apache-2.0', 'autotrain_compatible', 'endpoints_compatible', 'region:us'], 'pipeline_tag': 'image-classification', 'library_name': 'transformers', 'readme': '---\nlicense: apache-2.0\ntags:\n- vision\n- image-classification\n\ndatasets:\n- imagenet-1k\n\nwidget:\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/tiger.jpg\n  example_title: Tiger\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\n  example_title: Teapot\n- src: https://huggingface.co/datasets/mishig/sample_images/resolve/main/palace.jpg\n  example_title: Palace\n\n---\n\n# ResNet\n\nResNet model trained on imagenet-1k. It was introduced in the paper [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) and first released in [this repository](https://github.com/KaimingHe/deep-residual-networks). \n\nDisclaimer: The team releasing ResNet did not write a model card for this model so this model card has been written by the Hugging Face team.\n\n## Model description\n\nResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/resnet_architecture.png)\n\n## Intended uses & limitations\n\nYou can use the raw model for image classification. See the [model hub](https://huggingface.co/models?search=resnet) to look for\nfine-tuned versions on a task that interests you.\n\n### How to use\n\nHere is how to use this model:\n\n```python\n>>> from transformers import AutoImageProcessor, AutoModelForImageClassification\n>>> import torch\n>>> from datasets import load_dataset\n\n>>> dataset = load_dataset("huggingface/cats-image")\n>>> image = dataset["test"]["image"][0]\n\n>>> image_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-18")\n>>> model = AutoModelForImageClassification.from_pretrained("microsoft/resnet-18")\n\n>>> inputs = image_processor(image, return_tensors="pt")\n\n>>> with torch.no_grad():\n...     logits = model(**inputs).logits\n\n>>> # model predicts one of the 1000 ImageNet classes\n>>> predicted_label = logits.argmax(-1).item()\n>>> print(model.config.id2label[predicted_label])\ntiger cat\n```\n\n\n\nFor more code examples, we refer to the [documentation](https://huggingface.co/docs/transformers/master/en/model_doc/resnet).', 'model_index': None, 'widget_data': None, 'config': None, 'transformers_info': None, 'spaces': None, 'safetensors': None, 'used_storage': None, 'extracted_code': 'from transformers import AutoImageProcessor, AutoModelForImageClassification\nimport torch\nfrom datasets import load_dataset\n\ndataset = load_dataset("huggingface/cats-image")\nimage = dataset["test"]["image"][0]\n\nimage_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-18")\nmodel = AutoModelForImageClassification.from_pretrained("microsoft/resnet-18")\n\ninputs = image_processor(image, return_tensors="pt")\n\nwith torch.no_grad():\n    logits = model(**inputs).logits\n\n# model predicts one of the 1000 ImageNet classes\npredicted_label = logits.argmax(-1).item()\nprint(model.config.id2label[predicted_label])'}], 'datasets': [{'id': 'uoft-cs/cifar10', 'author': 'uoft-cs', 'sha': '0b2714987fa478483af9968de7c934580d0bb9a2', 'created_at': datetime.datetime(2022, 3, 2, 23, 29, 22, tzinfo=TzInfo(UTC)), 'last_modified': datetime.datetime(2024, 1, 4, 6, 53, 11, tzinfo=TzInfo(UTC)), 'private': False, 'gated': False, 'disabled': False, 'downloads': 77515, 'likes': 87, 'siblings': [{'rfilename': '.gitattributes', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'README.md', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'plain_text/test-00000-of-00001.parquet', 'size': None, 'blob_id': None, 'lfs': None}, {'rfilename': 'plain_text/train-00000-of-00001.parquet', 'size': None, 'blob_id': None, 'lfs': None}], 'card_data': {'license': ['unknown'], 'language': ['en'], 'library_name': None, 'pipeline_tag': None, 'tags': [], 'datasets': [], 'model_type': None, 'base_model': None, 'task_categories': ['image-classification'], 'size_categories': ['10K<n<100K'], 'metrics': [], 'widget': []}, 'tags': ['task_categories:image-classification', 'annotations_creators:crowdsourced', 'language_creators:found', 'multilinguality:monolingual', 'source_datasets:extended|other-80-Million-Tiny-Images', 'language:en', 'license:unknown', 'size_categories:10K<n<100K', 'format:parquet', 'modality:image', 'library:datasets', 'library:pandas', 'library:mlcroissant', 'library:polars', 'region:us'], 'pipeline_tag': None, 'library_name': None, 'readme': '---\nannotations_creators:\n- crowdsourced\nlanguage_creators:\n- found\nlanguage:\n- en\nlicense:\n- unknown\nmultilinguality:\n- monolingual\nsize_categories:\n- 10K<n<100K\nsource_datasets:\n- extended|other-80-Million-Tiny-Images\ntask_categories:\n- image-classification\ntask_ids: []\npaperswithcode_id: cifar-10\npretty_name: Cifar10\ndataset_info:\n  config_name: plain_text\n  features:\n  - name: img\n    dtype: image\n  - name: label\n    dtype:\n      class_label:\n        names:\n          \'0\': airplane\n          \'1\': automobile\n          \'2\': bird\n          \'3\': cat\n          \'4\': deer\n          \'5\': dog\n          \'6\': frog\n          \'7\': horse\n          \'8\': ship\n          \'9\': truck\n  splits:\n  - name: train\n    num_bytes: 113648310.0\n    num_examples: 50000\n  - name: test\n    num_bytes: 22731580.0\n    num_examples: 10000\n  download_size: 143646105\n  dataset_size: 136379890.0\nconfigs:\n- config_name: plain_text\n  data_files:\n  - split: train\n    path: plain_text/train-*\n  - split: test\n    path: plain_text/test-*\n  default: true\n---\n\n# Dataset Card for CIFAR-10\n\n## Table of Contents\n- [Dataset Description](#dataset-description)\n  - [Dataset Summary](#dataset-summary)\n  - [Supported Tasks and Leaderboards](#supported-tasks-and-leaderboards)\n  - [Languages](#languages)\n- [Dataset Structure](#dataset-structure)\n  - [Data Instances](#data-instances)\n  - [Data Fields](#data-fields)\n  - [Data Splits](#data-splits)\n- [Dataset Creation](#dataset-creation)\n  - [Curation Rationale](#curation-rationale)\n  - [Source Data](#source-data)\n  - [Annotations](#annotations)\n  - [Personal and Sensitive Information](#personal-and-sensitive-information)\n- [Considerations for Using the Data](#considerations-for-using-the-data)\n  - [Social Impact of Dataset](#social-impact-of-dataset)\n  - [Discussion of Biases](#discussion-of-biases)\n  - [Other Known Limitations](#other-known-limitations)\n- [Additional Information](#additional-information)\n  - [Dataset Curators](#dataset-curators)\n  - [Licensing Information](#licensing-information)\n  - [Citation Information](#citation-information)\n  - [Contributions](#contributions)\n\n## Dataset Description\n\n- **Homepage:** https://www.cs.toronto.edu/~kriz/cifar.html\n- **Repository:** \n- **Paper:** Learning Multiple Layers of Features from Tiny Images by Alex Krizhevsky\n- **Leaderboard:**\n- **Point of Contact:**\n\n### Dataset Summary\n\nThe CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\nThe dataset is divided into five training batches and one test batch, each with 10000 images. The test batch contains exactly 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class.\n\n### Supported Tasks and Leaderboards\n\n- `image-classification`: The goal of this task is to classify a given image into one of 10 classes. The leaderboard is available [here](https://paperswithcode.com/sota/image-classification-on-cifar-10).\n\n### Languages\n\nEnglish\n\n## Dataset Structure\n\n### Data Instances\n\nA sample from the training set is provided below:\n\n```\n{\n  \'img\': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32 at 0x201FA6EE748>,\n  \'label\': 0\n}\n```\n\n### Data Fields\n\n- img: A `PIL.Image.Image` object containing the 32x32 image. Note that when accessing the image column: `dataset[0]["image"]` the image file is automatically decoded. Decoding of a large number of image files might take a significant amount of time. Thus it is important to first query the sample index before the `"image"` column, *i.e.* `dataset[0]["image"]` should **always** be preferred over `dataset["image"][0]`\n- label: 0-9 with the following correspondence\n         0 airplane\n         1 automobile\n         2 bird\n         3 cat\n         4 deer\n         5 dog\n         6 frog\n         7 horse\n         8 ship\n         9 truck\n\n### Data Splits\n\nTrain and Test\n\n## Dataset Creation\n\n### Curation Rationale\n\n[More Information Needed]\n\n### Source Data\n\n#### Initial Data Collection and Normalization\n\n[More Information Needed]\n\n#### Who are the source language producers?\n\n[More Information Needed]\n\n### Annotations\n\n#### Annotation process\n\n[More Information Needed]\n\n#### Who are the annotators?\n\n[More Information Needed]\n\n### Personal and Sensitive Information\n\n[More Information Needed]\n\n## Considerations for Using the Data\n\n### Social Impact of Dataset\n\n[More Information Needed]\n\n### Discussion of Biases\n\n[More Information Needed]\n\n### Other Known Limitations\n\n[More Information Needed]\n\n## Additional Information\n\n### Dataset Curators\n\n[More Information Needed]\n\n### Licensing Information\n\n[More Information Needed]\n\n### Citation Information\n\n```\n@TECHREPORT{Krizhevsky09learningmultiple,\n    author = {Alex Krizhevsky},\n    title = {Learning multiple layers of features from tiny images},\n    institution = {},\n    year = {2009}\n}\n```\n\n### Contributions\n\nThanks to [@czabo](https://github.com/czabo) for adding this dataset.', 'model_index': None, 'widget_data': None, 'config': None, 'transformers_info': None, 'spaces': None, 'safetensors': None, 'used_storage': None, 'extracted_code': ''}]}}, 'experiment_code': None}

## Experiment Runs

- Run ID: proposed-ResNet-18-11.7-M--CIFAR-10
  Method: proposed
  Model: ResNet-18 (11.7 M)
  Dataset: CIFAR-10

- Run ID: comparative-1-ResNet-18-11.7-M--CIFAR-10
  Method: comparative-1
  Model: ResNet-18 (11.7 M)
  Dataset: CIFAR-10


# Requirements

## Configuration Structure
Each run configuration should include:
- run_id: Unique identifier for this run
- method: The method name (baseline, proposed, ablation, etc.)
- model: Model-specific parameters (name, architecture details, hyperparameters)
- dataset: Dataset-specific parameters (name, preprocessing settings, split ratios)
- training: Training hyperparameters (learning rate, batch size, epochs, optimizer settings)
- optuna: Hyperparameter search space definition for Optuna optimization
  - Define search spaces for key hyperparameters using Optuna's suggest methods
  - Example: learning_rate: [1e-5, 1e-3], batch_size: [16, 32, 64]
- Any other experiment-specific settings

## Format
- Generate one YAML configuration per experiment run
- Ensure valid YAML syntax
- Use meaningful parameter values based on the research method and experimental design

## Example Configuration
```yaml
run_id: baseline_bert_imdb
method: baseline
model:
  name: bert-base-uncased
  hidden_size: 768
  num_layers: 12
dataset:
  name: imdb
  max_length: 512
  batch_size: 32
training:
  learning_rate: 2e-5
  epochs: 3
  optimizer: adamw
  warmup_steps: 500
optuna:
  n_trials: 20
  search_space:
    learning_rate:
      type: loguniform
      low: 1e-5
      high: 1e-3
    batch_size:
      type: categorical
      choices: [16, 32, 64]
```

# Experimental Environment
NVIDIA A100×8
VRAM：80GB×8
RAM：2048 GB

# Instructions
1. Generate one YAML configuration for each experiment run listed above
2. Ensure configurations reflect the differences between baseline, proposed, and ablation methods
3. Use appropriate hyperparameters based on the experimental design
4. Include Optuna search space if hyperparameter optimization is beneficial for the experiment
5. For Optuna search spaces, use appropriate distribution types:
   - loguniform: For learning rates, regularization parameters
   - uniform: For dropout rates, weight decay
   - int: For hidden dimensions, number of layers
   - categorical: For discrete choices like batch size, optimizer type

Generate the configurations now:
Output:
{
  "run_configs": [
    {
      "run_id": "proposed-ResNet-18-11.7-M--CIFAR-10",
      "run_config_yaml": "run_id: proposed-ResNet-18-11.7-M--CIFAR-10\nmethod: proposed\nmodel:\n  name: ResNet-18\n  num_layers: 18\n  width: 64\n  pretrained: false\n  num_parameters: 11.7M\ndataset:\n  name: CIFAR-10\n  data_dir: ./data/cifar10\n  input_size: 32\n  num_classes: 10\n  augmentation:\n    random_crop:\n      size: 32\n      padding: 4\n    random_horizontal_flip: 0.5\n    normalization:\n      mean: [0.4914, 0.4822, 0.4465]\n      std:  [0.2470, 0.2435, 0.2616]\n  train_val_split: [0.9, 0.1]\ntraining:\n  optimizer: sgd\n  learning_rate: 0.1\n  momentum: 0.9\n  weight_decay: 5e-4\n  scheduler: cosine\n  batch_size: 256\n  epochs: 200\n  num_workers: 8\nalgorithm:\n  name: BOIL-UC\n  bo_iterations: 50\n  random_seeds: [0, 1, 2, 3, 4]\n  beta_uncert: 1.0\n  acquisition: \"log(EI) - log(mu_c + beta_uncert * sigma_c + 1e-6)\"\ncost_model:\n  type: BayesianRidge\n  alpha_1: 1e-6\n  lambda_1: 1e-6\noptuna:\n  n_trials: 30\n  direction: minimize\n  search_space:\n    beta_uncert:\n      type: categorical\n      choices: [0, 0.5, 1.0, 2.0]\n    alpha_1:\n      type: loguniform\n      low: 1e-7\n      high: 1e-5\n    lambda_1:\n      type: loguniform\n      low: 1e-7\n      high: 1e-5\n"
    },
    {
      "run_id": "comparative-1-ResNet-18-11.7-M--CIFAR-10",
      "run_config_yaml": "run_id: comparative-1-ResNet-18-11.7-M--CIFAR-10\nmethod: comparative-1\nmodel:\n  name: ResNet-18\n  num_layers: 18\n  width: 64\n  pretrained: false\n  num_parameters: 11.7M\ndataset:\n  name: CIFAR-10\n  data_dir: ./data/cifar10\n  input_size: 32\n  num_classes: 10\n  augmentation:\n    random_crop:\n      size: 32\n      padding: 4\n    random_horizontal_flip: 0.5\n    normalization:\n      mean: [0.4914, 0.4822, 0.4465]\n      std:  [0.2470, 0.2435, 0.2616]\n  train_val_split: [0.9, 0.1]\ntraining:\n  optimizer: sgd\n  learning_rate: 0.1\n  momentum: 0.9\n  weight_decay: 5e-4\n  scheduler: cosine\n  batch_size: 256\n  epochs: 200\n  num_workers: 8\nalgorithm:\n  name: BOIL\n  bo_iterations: 50\n  random_seeds: [0, 1, 2, 3, 4]\ncost_model:\n  type: LinearRegression\noptuna:\n  n_trials: 1\n  search_space: {}\n"
    }
  ]
}
